{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0616 01:47:04.358864 19356 file_utils.py:39] PyTorch version 1.2.0+cu92 available.\n",
      "C:\\Users\\danil\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import defaultdict\n",
    "from make_fn_data import load_fn_data\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics\n",
      "# data points:  200751\n",
      "# lex units without data:  3271\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "data = load_fn_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# frames:  1073\n"
     ]
    }
   ],
   "source": [
    "print(\"# frames: \", len(set([x[\"frame\"] for x in data])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datapoints from data\n",
    "\n",
    "frame_dict = {}\n",
    "frame_dict_rev = {}\n",
    "\n",
    "inputs = []\n",
    "labels = []\n",
    "\n",
    "for lu in data[:10]:\n",
    "    frame =  lu[\"frame\"]\n",
    "    if not frame in frame_dict.keys():\n",
    "        frame_dict[frame] = len(frame_dict.keys())\n",
    "        frame_dict_rev[frame_dict[frame]] = frame\n",
    "    frame_id = frame_dict[frame]\n",
    "    \n",
    "    for sentence in lu[\"sentences\"]:\n",
    "        text = sentence[\"text\"]\n",
    "        indexes = sentence[\"indexes\"]        \n",
    "        start = min([int(i[0]) for i in indexes])\n",
    "        end = max([int(i[1]) for i in indexes])\n",
    "        inputs.append((text, start, end))\n",
    "        labels.append(frame_id)\n",
    "        \n",
    "print(\"# datapoints = \", len(labels))\n",
    "print(\"max labels = \", max(labels))\n",
    "print(len(frame_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You should build your custom dataset as below.\n",
    "class NpClassDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, inputs, labels):\n",
    "        \"\"\"\n",
    "        arguments should be numpy arrays with shapes:\n",
    "        inputs: (N, F)\n",
    "        labels: (N, 1)\n",
    "        Where N = number of data points and F = number of features\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = torch.from_numpy(self.inputs[index].astype(np.float32))\n",
    "        y = torch.from_numpy(np.squeeze(self.labels[index]).astype(np.longlong))\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):        \n",
    "        return self.labels.shape[0]\n",
    "\n",
    "class Model():\n",
    "\n",
    "    def __init__(self, net, criterion, optimizer):\n",
    "        # try to move model to GPU, if exists\n",
    "        self.device = torch.device('cpu')\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "\n",
    "        self.net = net\n",
    "        self.net.to(self.device)\n",
    "        self.criterion = criterion \n",
    "        self.optimizer = optimizer\n",
    "        # used to compute probabilities\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def fit(self, dataset, n_epochs = 1, batch_size = 10, verbose=True, print_every=20):\n",
    "        # create data loader from data set\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # loop over the dataset multiple times\n",
    "        for epoch in range(n_epochs):\n",
    "            running_loss = 0.0\n",
    "            for i, (inputs, labels) in enumerate(data_loader, 0):\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = self.net(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                if verbose:\n",
    "                    # print statistics\n",
    "                    # prints every \"print_every\" mini-batches      \n",
    "                    if i % print_every == (print_every - 1):\n",
    "                        print('[%2d, %5d] loss: %.3f' %\n",
    "                              (epoch + 1, i + 1, running_loss / print_every))\n",
    "                        running_loss = 0.0\n",
    "        \n",
    "        print(\"Training finished\")\n",
    "\n",
    "\n",
    "    def test(self, dataset, batch_size=10):\n",
    "        label_correct = defaultdict(lambda: 0)\n",
    "        label_total = defaultdict(lambda: 0)\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for (inputs, labels) in data_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                predicted = self.predict(inputs)\n",
    "                \n",
    "                for (label, pred) in zip(labels.tolist(), predicted.tolist()):\n",
    "                    label_correct[label] += 1 if label == pred else 0\n",
    "                    label_total[label] += 1\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for label in label_correct.keys():\n",
    "            print(\"Label %s accuracy %d\\n\" %\n",
    "                (label, 100 * label_correct[label] / label_total[label]))\n",
    "            correct += label_correct[label]\n",
    "            total += label_total[label]\n",
    "\n",
    "        # TODO: only using accuracy, allow for other metrics\n",
    "        print('Total accuracy on %d data points: %d %%' % (\n",
    "            dataset.__len__(), 100 * correct / total))\n",
    "\n",
    "    def predict_dataset(self, dataset, batch_size=10):\n",
    "        predicted_lst = []\n",
    "        probs_lst = []\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=False)    \n",
    "        with torch.no_grad():\n",
    "            for (inputs, _) in data_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                predicted, probs = self.predict(inputs)\n",
    "                predicted_lst.append(predicted)\n",
    "        predicted_tensor = torch.cat(predicted_lst, 0)\n",
    "        probs_tensor = torch.cat(probs_lst, 0)\n",
    "        return predicted_tensor, probs_tensor\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.net(inputs)\n",
    "            probabilities = self.softmax(outputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            return predicted, probabilities[predicted]\n",
    "\n",
    "    def predict_top_k_dataset(dataset, k, batch_size=1):\n",
    "        predicted_lst = []\n",
    "        probs_lst = []\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=False)    \n",
    "        with torch.no_grad():\n",
    "            for (inputs, _) in data_loader:\n",
    "                predicted, probs = predict_top_k(inputs, k)\n",
    "                predicted_lst.append(predicted)\n",
    "                probs_lst.append(probs)\n",
    "        predicted_tensor = torch.cat(predicted_lst, 0)\n",
    "        probs_tensor = torch.cat(probs_lst, 0)\n",
    "        return predicted_tensor, probs_tensor\n",
    "    \n",
    "    def predict_top_k(inputs, k):\n",
    "        inputs = inputs.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = net(inputs)\n",
    "            logits, predicted = torch.topk(outputs.data, k, dim = 1)\n",
    "            probs = self.softmax(logits)\n",
    "            return predicted, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should build your custom dataset as below.\n",
    "class FnBertDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, inputs, labels, frame_dict, tokenizer, bert_model):\n",
    "        \"\"\"\n",
    "        First two arguments should be lists with the format:\n",
    "        inputs: [(text1, start1, end1), ...]\n",
    "        labels: [label_id1, ...]\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.bert_model = bert_model\n",
    "        \n",
    "        self.MAX_LEN = 3\n",
    "        self.INPUT_DIM = self.MAX_LEN * self.bert_model.config.hidden_size\n",
    "        self.OUTPUT_DIM = len(frame_dict.keys())\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text, start, end = self.inputs[index]\n",
    "        x = self.get_bert_hidden_state(text, start, end)\n",
    "        y = torch.tensor(self.labels[index]).long()        \n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def get_bert_hidden_state(self, text, start, end):\n",
    "        text = \"[CLS] \" + text + \" [SEP]\"\n",
    "        start += len(\"[CLS] \")\n",
    "        end += len(\"[CLS] \")\n",
    "        \n",
    "        # Compute start end end using token indexes\n",
    "        tk_start, tk_end = self.pos_to_token_idx(text, start, end)\n",
    "        tk_end = min(tk_start + self.MAX_LEN, tk_end)\n",
    "        # Tokenize input\n",
    "        tokenized_text = self.tokenizer.tokenize(text)\n",
    "    \n",
    "        # Convert token to vocabulary indices\n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([indexed_tokens]).to('cuda')\n",
    "        # Predict hidden states features for each layer\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(tokens_tensor)\n",
    "            # Hidden state of the last layer of the Bert model\n",
    "            hidden = torch.squeeze(outputs[0], dim = 0)\n",
    "            # Slice hidden state to hidden[start:end]\n",
    "            hidden = hidden.narrow(0, tk_start, tk_end-tk_start)\n",
    "            # Add padding\n",
    "            pad = torch.zeros(self.MAX_LEN, hidden.size()[1])            \n",
    "            pad[0:hidden.size()[0],:] = hidden\n",
    "            hidden = torch.flatten(pad)\n",
    "            return hidden\n",
    "\n",
    "    def pos_to_token_idx(self, text, start, end):\n",
    "        target_prefix = self.tokenizer.tokenize(text[:start])\n",
    "        target = self.tokenizer.tokenize(text[start:end+1])\n",
    "        tk_start = len(target_prefix)\n",
    "        tk_end = tk_start + len(target)\n",
    "        return tk_start, tk_end\n",
    "    \n",
    "bert_model.to('cuda')\n",
    "# dataset = FnBertDataset([inputs[0], inputs[-1]], [labels[0], labels[-1]], frame_dict, tokenizer, bert_model)\n",
    "dataset = FnBertDataset(inputs, labels, frame_dict, tokenizer, bert_model)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "for i in range(10):\n",
    "    dataset[i]\n",
    "print(\"elapsed time seconds = \", time.time() - start_time)\n",
    "\n",
    "print(\"dataset in = \", dataset[0][0])\n",
    "print(\"dataset out = \", dataset[0][1], dataset[0][1].type())\n",
    "print(\"dimensions: in =\", dataset.INPUT_DIM, \" out = \", dataset.OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_net(input_dim, output_dim):\n",
    "    layers = [\n",
    "        nn.Linear(input_dim, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, output_dim),    \n",
    "    ]\n",
    "    model = nn.Sequential(*layers)\n",
    "    return model\n",
    "\n",
    "# Run training & testing\n",
    "net = create_net(input_dim = dataset.INPUT_DIM, output_dim = dataset.OUTPUT_DIM)\n",
    "model = Model(net, criterion = nn.CrossEntropyLoss(),\n",
    "              optimizer=optim.Adam(net.parameters(), lr=10e-4))\n",
    "model.fit(dataset, n_epochs=5, batch_size=10, verbose=True, print_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    net.state_dict(), 'C:\\\\Users\\\\danil\\\\Documents\\\\Northwestern\\\\Research\\\\projects\\\\frame_classification\\\\state_dict_small')\n",
    "torch.save(\n",
    "    net, 'C:\\\\Users\\\\danil\\\\Documents\\\\Northwestern\\\\Research\\\\projects\\\\frame_classification\\\\net_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = FnBertDataset(inputs, labels, frame_dict, tokenizer, bert_model)\n",
    "# dev_dataset = FnBertDataset([inputs[0], inputs[-1]], [labels[0], labels[-1]], \n",
    "#                             frame_dict, tokenizer, bert_model)\n",
    "print(len(dev_dataset))\n",
    "model.test(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frame_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
